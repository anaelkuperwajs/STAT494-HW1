---
title: 'Assignment #1'
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_download: true
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r libraries}
library(tidyverse)         # for graphing and data cleaning
library(tidymodels)        # for modeling
library(naniar)            # for analyzing missing values
library(vip)               # for variable importance plots
theme_set(theme_minimal()) # Lisa's favorite theme
```

```{r data}
hotels <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv')
```


When you finish the assignment, remove the `#` from the options chunk at the top, so that messages and warnings aren't printed. If you are getting errors in your code, add `error = TRUE` so that the file knits. I would recommend not removing the `#` until you are completely finished.

## Setting up Git and GitHub in RStudio

Read the [Quick Intro](https://advanced-ds-in-r.netlify.app/posts/2021-01-28-gitgithub/#quick-intro) section of the Using git and GitHub in R Studio set of Course Materials. Set up Git and GitHub and create a GitHub repo and associated R Project (done for you when you clone the repo) for this homework assignment. Put this file into the project. You should always open the R Project (.Rproj) file when you work with any of the files in the project.

**Task**: Below, post a link to your GitHub repository.
https://github.com/anaelkuperwajs/STAT494-HW1

## Creating a website

You'll be using RStudio to create a personal website to showcase your work from this class! Start by watching the [Sharing on Short Notice](https://rstudio.com/resources/webinars/sharing-on-short-notice-how-to-get-your-materials-online-with-r-markdown/) webinar by Alison Hill and DesirÃ©e De Leon of RStudio. This should help you choose the type of website you'd like to create.

Once you've chosen that, you might want to look through some of the other *Building a website* resources I posted on the [resources page](https://advanced-ds-in-r.netlify.app/resources.html) of our course website. I highly recommend making a nice landing page where you give a brief introduction of yourself.


**Tasks**:

* Include a link to your website below. (If anyone does not want to post a website publicly, please talk to me and we will find a different solution).

Website link:

* Listen to at least the first 20 minutes of "Building a Career in Data Science, Chapter 4: Building a Portfolio". Go to the main [podcast website](https://podcast.bestbook.cool/) and navigate to a podcast provider that works for you to find that specific episode. Write 2-3 sentences reflecting on what they discussed and why creating a website might be helpful for you.

REFLECTION

* (Optional) Create an R package with your own customized `gpplot2` theme! Write a post on your website about why you made the choices you did for the theme. See the *Building an R package* and *Custom `ggplot2` themes* [resources](https://advanced-ds-in-r.netlify.app/resources.html).

## Machine Learning review and intro to `tidymodels`

Read through and follow along with the [Machine Learning review with an intro to the `tidymodels` package](https://advanced-ds-in-r.netlify.app/posts/2021-03-16-ml-review/) posted on the Course Materials page. 

**Tasks**:

1. Read about the hotel booking data, `hotels`, on the [Tidy Tuesday page](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md) it came from. There is also a link to an article from the original authors. The outcome we will be predicting is called `is_canceled`. 
  - Without doing any analysis, what are some variables you think might be predictive and why?
  
I believe previous_cancellations and previous_bookings_not_canceled coudl be indicative because they both have data on a customer's history, and past actions can help predict future actions. In addition, total_of_special_requests could be predictive because different types of requests might be more likely to cancel, especially if some have higher cancellation fees. Furthermore, the longer in advance a booking is made, the more likely plans are to change, so lead_time could be predictive. Another thing that could be indicative is the customer_type, because some types of customers might be more reliable than others. Lastly, babies could be predictive because it is more likley that plans might change when someone has a baby.
  
  _ What are some problems that might exist with the data? You might think about how it was collected and who did the collecting.
  
A lack of variety in the hotels could be problematic because it is less generalizable. This dataset only used two hotels. The data also comes from an open hotel booking demand dataset, rather than a hotel itself, which might indicate some differences. In addition, some of the data is private, not all of it is public, which might affect the accessibility later on.
  
  - If we construct a model, what type of conclusions will be able to draw from it?
  
The model might help us figure out what are the best predictors of if a booking will be canceled. It could also predict the probability of a specific booking being canceled, which might help hotels predict profit and budget more effectively.
  
2. Create some exploratory plots or table summaries of the data.

One thing that stands out is that babies and previous_cancellations has a lot of entries at the value 0 and very few other than 0.

```{r}
ggplot(hotels, aes(x = previous_cancellations)) +
  geom_bar()

ggplot(hotels, aes(x = total_of_special_requests)) +
  geom_bar()

ggplot(hotels, aes(x = babies)) +
  geom_bar()

ggplot(hotels, aes(x = previous_cancellations, y = total_of_special_requests)) +
  geom_jitter()

ggplot(hotels, aes(x = previous_cancellations, y = as.factor(total_of_special_requests))) +
  geom_boxplot()

hotels %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(cols = everything(),
               names_to = "variable", 
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30) +
  facet_wrap(vars(variable), 
             scales = "free")
```


3. First, we will do a couple things to get the data ready, including making the outcome a factor (needs to be that way for logistic regression), removing the year variable and some reservation status variables, and removing missing values (not NULLs but true missing values).

Split the data into a training and test set, stratifying on the outcome variable, `is_canceled`. Since we have a lot of data, we're going to split the data 50/50 between training and test. I have already `set.seed()` for you. Be sure to use `hotels_mod` in the splitting.

```{r}
hotels_mod <- hotels %>% 
  mutate(is_canceled = as.factor(is_canceled)) %>% 
  mutate(across(where(is.character), as.factor)) %>% 
  select(-arrival_date_year,
         -reservation_status,
         -reservation_status_date) %>% 
  add_n_miss() %>% 
  filter(n_miss_all == 0) %>% 
  select(-n_miss_all)

set.seed(494)
hotels_split <- initial_split(hotels_mod, prop = 1/2, strata = is_canceled)
hotels_split

hotels_training <- training(hotels_split)
hotels_testing <- testing(hotels_split)
```

4. In this next step, we are going to do the pre-processing. Usually, I won't tell you exactly what to do here, but for your first exercise, I'll tell you the steps. 

* Set up the recipe with `is_canceled` as the outcome and all other variables as predictors (HINT: `~.`).
* Use a `step_XXX()` function or functions (I think there are other ways to do this, but I found `step_mutate_at()` easiest) to create some indicator variables for the following variables: `children`, `babies`, and `previous_cancellations`. So, the new variable should be a 1 if the original is more than 0 and 0 otherwise. Make sure you do this in a way that accounts for values that may be larger than any we see in the dataset.
* For the `agent` and `company` variables, make new indicator variables that are 1 if they have a value of `NULL` and 0 otherwise.
* Use `fct_lump_n()` to lump together countries that aren't in the top 5 most occurring. 
* If you used new names for some of the new variables you created, then remove any variables that are no longer needed. 
* Use `step_normalize()` to center and scale all the non-categorical predictor variables. (Do this BEFORE creating dummy variables. When I tried to do it after, I ran into an error - I'm still investigating why.)
* Create dummy variables for all factors/categorical predictor variables (make sure you have `-all_outcomes()` in this part!!).  
* Use the `prep()` and `juice()` functions to apply the steps to the training data just to check that everything went as planned.

```{r}
hotels_recipe <- recipe(is_canceled ~ ., data = hotels_training) %>%
  step_mutate_at(children, babies, previous_cancellations, fn = ~as.factor((. > 0))) %>% #if something breaks down change to as.numeric
  step_mutate_at(agent, company, fn = ~as.factor(. == "NULL")) %>% 
  step_mutate(country = fct_lump_n(country, 5)) %>% 
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes())

prep(hotels_recipe) %>% 
  juice()
```

5. In this step we will set up a LASSO model and workflow.

* In general, why would we want to use LASSO instead of regular logistic regression? (HINT: think about what happens to the coefficients).

The purpose of LASSO is to obtain the subset of predictors that minimizes prediction error, so that is useful for this model, where not all of the predictors would be useful for predicting cancellation.

* Define the model type, set the engine, set the `penalty` argument to `tune()` as a placeholder, and set the mode.
* Create a workflow with the recipe and model.

```{r}
hotels_lasso_mod <- 
  logistic_reg(mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>%
  set_mode("classification")

hotels_lasso_mod

hotels_lasso_wf <- 
  # Set up the workflow
  workflow() %>% 
  # Add the recipe
  add_recipe(hotels_recipe) %>% 
  # Add the modeling
  add_model(hotels_lasso_mod)

hotels_lasso_wf
```


6. In this step, we'll tune the model and fit the model using the best tuning parameter to the entire training dataset.

* Create a 5-fold cross-validation sample. We'll use this later. I have set the seed for you.  
* Use the `grid_regular()` function to create a grid of 10 potential penalty parameters (we're keeping this sort of small because the dataset is pretty large). Use that with the 5-fold cv data to tune the model.
* Use the `tune_grid()` function to fit the models with different tuning parameters to the different cross-validation sets.
* Use the `collect_metrics()` function to collect all the metrics from the previous step and create a plot with the accuracy on the y-axis and the penalty term on the x-axis. Put the x-axis on the log scale.  
* Use the `select_best()` function to find the best tuning parameter, fit the model using that tuning parameter to the entire training set (HINT: `finalize_workflow()` and `fit()`), and display the model results using `pull_workflow_fit()` and `tidy()`. Are there some variables with coefficients of 0?

Yes, there are a few coefficients, including arrival_date_month_September, market_segment_Groups, market_segment_Undefined, distribution_channel_Undefined, and assigned_room_type_L.

```{r}
set.seed(494) # for reproducibility

hotels_cv <- vfold_cv(hotels_training, v = 5)

penalty_grid <- grid_regular(penalty(), levels = 10)
penalty_grid 

hotels_lasso_tune <- 
  hotels_lasso_wf %>% 
  tune_grid(
    resamples = hotels_cv,
    grid = penalty_grid
    )

hotels_lasso_tune

hotels_lasso_tune %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  scale_x_log10(
   breaks = scales::trans_breaks("log10", function(x) 10^x),
   labels = scales::trans_format("log10",scales::math_format(10^.x))) +
  labs(x = "penalty", y = "accuracy")

best_param <- hotels_lasso_tune %>% 
  select_best(metric = "accuracy")
best_param

hotels_lasso_final_wf <- hotels_lasso_wf %>% 
  finalize_workflow(best_param)
hotels_lasso_final_wf

hotels_lasso_final_mod <- hotels_lasso_final_wf %>% 
  fit(data = hotels_training)

hotels_lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  tidy()
```


7. Now that we have a model, let's evaluate it a bit more. All we have looked at so far is the cross-validated accuracy from the previous step. 

* Create a variable importance graph. Which variables show up as the most important? Are you surprised?

The variables that show up as most important are reserved_room_type_P, deposit_type_Non.Refund, assigned_room_type_I, previous_cancellations_TRUE, followed by a few different assigned or reserved room types. I am surprised, because many of the variables that I thought would be important are not on the list at all, and I did not expect assigned or reserved room type to be that important. The deposit type and the previous cancellations as important variables is understandable.

* Use the `last_fit()` function to fit the final model and then apply it to the testing data. Report the metrics from the testing data using the `collet_metrics()` function. How do they compare to the cross-validated metrics?

The estimate is 0.8154055, while there are ten values for cross-validated mean, which range from 0.6296048 to six values of 0.8136130. The majority are similar to the testing data metrics, only off by approximately 0.002.

* Use the `collect_predictions()` function to find the predicted probabilities and classes for the test data. Save this to a new dataset called `preds`. Then, use the `conf_mat()` function from `dials` (part of `tidymodels`) to create a confusion matrix showing the predicted classes vs. the true classes. What is the true positive rate (sensitivity)? What is the true negative rate (specificity)? See this [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix) reference if you (like me) tend to forget these definitions.

The sensitivity (true positive rate) is = 14377 / (14377 + 7733) = 0.6502
The specificity (true negative rate) is = 34297 / (34297 + 3286) = 0.9126

The value of 1 means the booking is canceled, so that would be the positive value. The negative value is 0, when the booking is not canceled.

* Use the `preds` dataset you just created to create a density plot of the predicted probabilities of canceling (the variable is called `.pred_1`), filling by `is_canceled`. Use an `alpha = .5` and `color = NA` in the `geom_density()`. Answer these questions: a. What would this graph look like for a model with an accuracy that was close to 1? b. Our predictions are classified as canceled if their predicted probability of canceling is greater than .5. If we wanted to have a high true positive rate, should we make the cutoff for predicted as canceled higher or lower than .5? c. What happens to the true negative rate if we try to get a higher true positive rate? 

a. If the accuracy was higher than the two parts of the graph would overlap less.

b. If we wanted a high true positive rate, we should make the cutoff for predicted as canceled lower than .5.

c. The true negative rate would be worse if we aim for a higher true positive rate.

```{r}
hotels_lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  vip()

# Fit model with best tuning parameter(s) to training data and apply to test data
hotels_lasso_test <- hotels_lasso_final_wf %>% 
  last_fit(hotels_split)

# Metrics for model applied to test data
hotels_lasso_test %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy")

preds <- collect_predictions(hotels_lasso_test)
preds

conf_mat(preds, truth = is_canceled, estimate = .pred_class)

ggplot(preds, aes(x = .pred_1, fill = is_canceled)) +
  geom_density(alpha = .5, color = NA)
```


8. Let's say that this model is going to be applied to bookings 14 days in advance of their arrival at each hotel, and someone who works for the hotel will make a phone call to the person who made the booking. During this phone call, they will try to assure that the person will be keeping their reservation or that they will be canceling in which case they can do that now and still have time to fill the room. How should the hotel go about deciding who to call? How could they measure whether it was worth the effort to do the calling? Can you think of another way they might use the model? 

One way of deciding who would be best to call is by looking the at important variables that the model suggested. While some of the variables didn't necessarily make sense, prioritizing calling people who have had previous cancellations or did not have to place a deposit, since that might be likely to indicate future cancellations. Another way would be to raise the true negative rate by increasing the cutoff so that the effort is more directed towards people who are more likely to cancel a booking. They could measure if the effort was worth it by seeing how many people still canceled after the phone call and how many canceled during the phone call vs the people who did not get a phone call. Since the model indicates which variables best predict a future cancellation or not and many of the important predictors were about assigned or reserved room type, it could indicate which rooms are considered more valuable or less valuable by the guests. This could help determine which are the most valued room types. Another way to use the model would be to know which variables do not predict cancellations and aren't as helpful.

9. How might you go about questioning and evaluating the model in terms of fairness? Are there any questions you would like to ask of the people who collected the data? 

Possible ways of evaluating fairness of a model is to check how identity plays a role in the model. For example, looking to see if there are differences in accuracy when looking at female vs male guests. If the accuracy is much higher for male guests, there might be an issue in fairness. A few in this dataset that might lead to biased predictions is country, customer_type, babies or children, or company. If the model is biased towards people affilitated with a specific company, then it might lead to unfair treatment of individual guests. I would want to ask the people who collected the data if integrity, fairness, or bias was on their mind when they collected the data. Furthermore, understanding how the data was collected and how the sample of hotels was chosen would help. There were only two hotels in the dataset, which already leads to some problems with generalizability.


## Bias and Fairness

Listen to Dr. Rachel Thomas's  [Bias and Fairness lecture](https://ethics.fast.ai/videos/?lesson=2). Write a brief paragraph reflecting on it. You might also be interested in reading the [ProPublica article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) Dr. Thomas references about using a tool called COMPAS to predict recidivism. Some questions/ideas you might keep in mind:

* Did you hear anything that surprised you?  
* Why is it important that we pay attention to bias and fairness when studying data science?  
* Is there a type of bias Dr. Thomas discussed that was new to you? Can you think about places you have seen these types of biases?


